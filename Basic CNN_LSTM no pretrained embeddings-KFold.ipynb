{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.tests.test_cross_validation import test_train_test_split\n",
    "import pandas as pd\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 100\n",
    "\n",
    "# Convolution\n",
    "filter_length = 2\n",
    "nb_filter = 10\n",
    "pool_length = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 5\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "test__train_split_ratio = 0.1# 0.1 means 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(corpus, labels):\n",
    "\t# Randomize the dataSet\n",
    "\trandom_data = []\n",
    "\trandom_labels = []\n",
    "\t\n",
    "\t# Sample indices from 0..len(corpus)\n",
    "\tsize = len(corpus)\n",
    "\trand_indices = random.sample(range(size), size)\n",
    "\t\n",
    "\t\n",
    "\t# Insert in the final dataset N=self.datasetSize random tweets from the rawData\n",
    "\tfor index in rand_indices:\n",
    "\t\trandom_data.append(corpus[index])\n",
    "\t\trandom_labels.append(labels[index])\n",
    "\t\n",
    "\t# Calculate the test set size\n",
    "\ttest_set_size = int(test__train_split_ratio * size)\n",
    "\t\n",
    "\t# The trainSet starts from the begining until before the end by the test set size \n",
    "\ttrain_set = random_data[0 : size - test_set_size]\n",
    "\ttest_set  = random_data[len(train_set) : size]\n",
    "\ttrain_set_labels = random_labels[0 : size - test_set_size]\n",
    "\ttest_set_labels  = random_labels[len(train_set) : size]\t\n",
    "\treturn train_set, train_set_labels, test_set, test_set_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_to_indices(text):\n",
    "\t#words_map = build_words_map(text)\n",
    "\t\t\n",
    "\t#text_to_indices(text, words_map)\n",
    "\n",
    "\t# The vocabulary map\n",
    "\twords_map = {}\n",
    "\n",
    "\t# Index of words\n",
    "\tindex = 0\n",
    "\t\n",
    "\t# Initialize the output list\n",
    "\ttext_indices = []\n",
    "\tmaxlen = 0\n",
    "\t# Loop line by line\n",
    "\t\n",
    "\tfor line in text:\n",
    "\n",
    "\t\t# Split into words\t\n",
    "\t\ttry:\t\t\n",
    "\t\t\tline = str(line)\n",
    "\t\t\tline_words = line.split()\n",
    "\t\texcept:\n",
    "\t\t\tprint(str(line))\n",
    "\t\n",
    "\t\tif len(line_words) > maxlen:\n",
    "\t\t\tmaxlen = len(line_words) \n",
    "\t\t# Initialize the line_indices\n",
    "\t\tline_indices = []\n",
    "\t\t# Loop word by word\n",
    "\t\tfor word in line_words:\n",
    "\t\t\t# Store the word once in the wordMap\n",
    "\t\t\tif not word in words_map:\n",
    "\t\t\t\twords_map[word] = index\n",
    "\t\t\t\t# Increment the index for the next word\n",
    "\t\t\t\tindex += 1\n",
    "\n",
    "\t\t\t# Add the index to the line_indices\n",
    "\t\t\tline_indices.append(words_map[word])\n",
    "\n",
    "\t\t# Add the line_indices to the output list\n",
    "\t\ttext_indices.append(line_indices)\n",
    "\n",
    "\n",
    "\treturn text_indices, len(words_map), maxlen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "\t# Load training data frame\n",
    "\ttrain_data_path = '..\\\\..\\\\dat\\\\train.tsv'\n",
    "\ttrain_df = pd.read_table(train_data_path)\n",
    "\tprint('Training DataFrame loaded')\n",
    "\t\n",
    "\ttext = train_df['item_description']\n",
    "\ttext_indices, voc_size, maxlen = corpus_to_indices(text)\n",
    "\t\n",
    "\t# Load labels\n",
    "\tlabels = np.array(train_df['price'])\n",
    "\t\n",
    "\t#X_train, y_train, X_test, y_test = split_train_test(text_indices, labels) \n",
    "    #return X_train, y_train, X_test, y_test, voc_size, maxlen\n",
    "\treturn text_indices, labels, voc_size, maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "X, Y, max_features, maxlen = load_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('X shape:', X.shape)\n",
    "\n",
    "\n",
    "Y = np.reshape(Y, [len(Y), 1])\n",
    "\n",
    "\n",
    "print('Y shape:', Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Build model...')\n",
    "def model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    #model.add(Activation('sigmoid'))\n",
    "\n",
    "    import keras.optimizers\n",
    "    opt = keras.optimizers.adam(0.01)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    model.save('CNN_LSTM_no_glove_KFold_model', overwrite=True)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "print('Train...')\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=model, nb_epoch=100, batch_size=5, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
