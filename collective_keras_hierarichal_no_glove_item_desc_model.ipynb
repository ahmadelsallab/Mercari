{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import math\n",
    "from nltk import tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded\n",
      "(1482535, 8)\n",
      "(693359, 7)\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_table(\"../../dat/train.tsv\")\n",
    "test = pd.read_table(\"../../dat/test.tsv\")\n",
    "print('Data loaded')\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "(1482535, 8)\n",
      "(693359, 7)\n"
     ]
    }
   ],
   "source": [
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Tops/T-shirts</td>\n",
       "      <td>missing</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>Electronics/Computers &amp; Tablets/Components &amp; P...</td>\n",
       "      <td>Razer</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/Blouse</td>\n",
       "      <td>Target</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "\n",
       "                                       category_name brand_name  price  \\\n",
       "0                                  Men/Tops/T-shirts    missing     10   \n",
       "1  Electronics/Computers & Tablets/Components & P...      Razer     52   \n",
       "2                        Women/Tops & Blouses/Blouse     Target     10   \n",
       "\n",
       "   shipping                                   item_description  \n",
       "0         1                                 No description yet  \n",
       "1         0  This keyboard is in great condition and works ...  \n",
       "2         1  Adorable top with a hint of lace and a key hol...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling categorical variables...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>829</td>\n",
       "      <td>5265</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>3889</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>1277</td>\n",
       "      <td>4588</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "\n",
       "   category_name  brand_name  price  shipping  \\\n",
       "0            829        5265     10         1   \n",
       "1             86        3889     52         0   \n",
       "2           1277        4588     10         1   \n",
       "\n",
       "                                    item_description  \n",
       "0                                 No description yet  \n",
       "1  This keyboard is in great condition and works ...  \n",
       "2  Adorable top with a hint of lace and a key hol...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PROCESS CATEGORICAL DATA\n",
    "print(\"Handling categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train.category_name = le.transform(train.category_name)\n",
    "test.category_name = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train.brand_name = le.transform(train.brand_name)\n",
    "test.brand_name = le.transform(test.brand_name)\n",
    "del le\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to seq process...\n",
      "   Transforming text to seq of sentences, which is a sequence of words...\n",
      "   Fitting tokenizer...\n",
      "Total 259198 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n",
    "print(\"   Transforming text to seq of sentences, which is a sequence of words...\")\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsent_words_indices = tokenizer.texts_to_sequences(sentences)\\n\\nprint(sent_words_indices)\\nfor j in range(len(sent_words_indices)):\\n    if j< MAX_SENTS:\\n        for k in range(len(sent_words_indices[j])):\\n            word_idx = sent_words_indices[j][k]\\n            if k < MAX_SENT_LENGTH and word_idx < MAX_NB_WORDS:\\n                print(word_idx)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "MAX_SENTS = 5\n",
    "MAX_SENT_LENGTH = 25\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "desc = train.item_description.str.lower()\n",
    "desc[14]\n",
    "sentences = tokenize.sent_tokenize(desc[14])\n",
    "#sentences = tokenize.sent_tokenize(desc[14])\n",
    "print(sentences)\n",
    "'''\n",
    "'''\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    sent_words_indices = tokenizer.texts_to_sequences(sent)\n",
    "    for word_idx in sent_words_indices:\n",
    "            print word_idx\n",
    "    #print(sent_words_indices)\n",
    "'''\n",
    "'''\n",
    "sent_words_indices = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(sent_words_indices)\n",
    "for j in range(len(sent_words_indices)):\n",
    "    if j< MAX_SENTS:\n",
    "        for k in range(len(sent_words_indices[j])):\n",
    "            word_idx = sent_words_indices[j][k]\n",
    "            if k < MAX_SENT_LENGTH and word_idx < MAX_NB_WORDS:\n",
    "                print(word_idx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_SENTS = 5\n",
    "MAX_SENT_LENGTH = 25\n",
    "MAX_NB_WORDS = 20000\n",
    "def process_hierarichal_text(descriptions_raw):\n",
    "    descriptions = []\n",
    "    for description in descriptions_raw:\n",
    "        \n",
    "        sentences = tokenize.sent_tokenize(description.decode('utf8'))        \n",
    "        \n",
    "        #sentences = tokenize.sent_tokenize(description)\n",
    "        \n",
    "        '''\n",
    "        MAX_SENTS = max(MAX_SENTS, len(sentences))\n",
    "        for sent in sentences:\n",
    "            MAX_SENT_LENGTH = max(MAX_SENT_LENGTH, len(tok_raw.texts_to_sequences(sent)))\n",
    "        '''\n",
    "        descriptions.append(sentences)\n",
    "    #print('MAX_SENTS = ', MAX_SENTS)\n",
    "    #print('MAX_SENT_LENGTH = ', MAX_SENT_LENGTH)\n",
    "\n",
    "    data = np.zeros((len(descriptions_raw), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "    #n_errs = 0\n",
    "    for i, description in enumerate(descriptions):\n",
    "        sent_words_indices = tokenizer.texts_to_sequences(description)\n",
    "        for j in range(len(sent_words_indices)):\n",
    "            if j< MAX_SENTS:\n",
    "                for k in range(len(sent_words_indices[j])):\n",
    "                    word_idx = sent_words_indices[j][k]\n",
    "                    if k < MAX_SENT_LENGTH and word_idx < MAX_NB_WORDS:\n",
    "                        data[i,j,k] = word_idx\n",
    "                        #print(data[i,j,k])\n",
    "                    \n",
    "    #print('Total errors=', n_errs)\n",
    "    #print(data)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 259198 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s unique tokens.' % len(word_index))\n",
    "'''\n",
    "train_short = train['item_description'][1:10]\n",
    "train_short = train_short.str.lower()\n",
    "train_seq_item_desc = process_hierarichal_text(train_short)\n",
    "print(train_seq_item_desc.shape)\n",
    "print(train_short)\n",
    "train_short[\"seq_item_description\"] = list(train_seq_item_desc)\n",
    "'''\n",
    "train_seq_item_desc = process_hierarichal_text(train.item_description.str.lower())\n",
    "test_seq_item_desc = process_hierarichal_text(test.item_description.str.lower())\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "train[\"seq_item_description\"] = list(train_seq_item_desc)\n",
    "test[\"seq_item_description\"] = list(test_seq_item_desc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>seq_item_description</th>\n",
       "      <th>seq_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MLB Cincinnati Reds T Shirt Size XL</td>\n",
       "      <td>3</td>\n",
       "      <td>829</td>\n",
       "      <td>5265</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>No description yet</td>\n",
       "      <td>[[12, 68, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[3851, 8822, 6896, 208, 84, 6, 155]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Razer BlackWidow Chroma Keyboard</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>3889</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>This keyboard is in great condition and works ...</td>\n",
       "      <td>[[29, 2627, 10, 7, 39, 17, 1, 207, 51, 19, 111...</td>\n",
       "      <td>[10759, 25570, 16366, 2627]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AVA-VIV Blouse</td>\n",
       "      <td>1</td>\n",
       "      <td>1277</td>\n",
       "      <td>4588</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Adorable top with a hint of lace and a key hol...</td>\n",
       "      <td>[[604, 60, 9, 4, 5347, 11, 192, 1, 4, 886, 129...</td>\n",
       "      <td>[7634, 10563, 666]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                 name  item_condition_id  \\\n",
       "0         0  MLB Cincinnati Reds T Shirt Size XL                  3   \n",
       "1         1     Razer BlackWidow Chroma Keyboard                  3   \n",
       "2         2                       AVA-VIV Blouse                  1   \n",
       "\n",
       "   category_name  brand_name  price  shipping  \\\n",
       "0            829        5265     10         1   \n",
       "1             86        3889     52         0   \n",
       "2           1277        4588     10         1   \n",
       "\n",
       "                                    item_description  \\\n",
       "0                                 No description yet   \n",
       "1  This keyboard is in great condition and works ...   \n",
       "2  Adorable top with a hint of lace and a key hol...   \n",
       "\n",
       "                                seq_item_description  \\\n",
       "0  [[12, 68, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [[29, 2627, 10, 7, 39, 17, 1, 207, 51, 19, 111...   \n",
       "2  [[604, 60, 9, 4, 5347, 11, 192, 1, 4, 886, 129...   \n",
       "\n",
       "                              seq_name  \n",
       "0  [3851, 8822, 6896, 208, 84, 6, 155]  \n",
       "1          [10759, 25570, 16366, 2627]  \n",
       "2                   [7634, 10563, 666]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"seq_name\"] = tokenizer.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tokenizer.texts_to_sequences(test.name.str.lower())\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings size calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max name seq 17\n",
      "max item desc seq 5\n"
     ]
    }
   ],
   "source": [
    "#SEQUENCES VARIABLES ANALYSIS\n",
    "max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\n",
    "max_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n",
    "                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\n",
    "print(\"max name seq \"+str(max_name_seq))\n",
    "print(\"max item desc seq \"+str(max_seq_item_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 10\n",
    "#MAX_ITEM_DESC_SEQ = 75\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category_name.max(), test.category_name.max()])+1\n",
    "MAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained embeddings (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fefdacaa810>]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefdacaaf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SCALE target variable\n",
    "train[\"target\"] = np.log(train.price+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\n",
    "pd.DataFrame(train.target).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1467709, 11)\n",
      "(14826, 11)\n"
     ]
    }
   ],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    seq_item_description_data = np.reshape(list(dataset.seq_item_description), [len(dataset.seq_item_description), MAX_SENTS,MAX_SENT_LENGTH])\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': seq_item_description_data\n",
    "        ,'brand_name': np.array(dataset.brand_name)\n",
    "        ,'category_name': np.array(dataset.category_name)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "category_name (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_desc (InputLayer)          (None, 5, 25)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name (InputLayer)               (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        52900       brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 10)        13110       category_name[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 5)         30          item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 5, 32)        12968526    item_desc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 50)       12959950    name[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 5)            0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 16)           2624        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 8)            1416        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 50)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 gru_1[0][0]                      \n",
      "                                                                 num_vars[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          6528        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            65          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,013,405\n",
      "Trainable params: 26,013,405\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.utils.vis_utils import plot_model\\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "def rmsle_cust(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "def get_hierarichal_representation(item_desc_input_layer):\n",
    "    \n",
    "    # building Hierachical Attention network\n",
    "    EMBEDDING_DIM = 50\n",
    "    # Pre-trained Embeddings\n",
    "    '''\n",
    "    GLOVE_DIR = \"../data/glove\"\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embeddings_index))    \n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SENT_LENGTH,\n",
    "                                trainable=True)\n",
    "    '''\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "    # Attention\n",
    "    '''\n",
    "    class AttLayer(Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            self.init = initializations.get('normal')\n",
    "            super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            assert len(input_shape)==3\n",
    "            self.W = self.init((input_shape[-1],1))\n",
    "            self.trainable_weights = [self.W]\n",
    "            super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "        def call(self, x, mask=None):\n",
    "            eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "            ai = K.exp(eij)\n",
    "        weights = ai/tf.expand_dims(K.sum(ai, axis=1), 1)\n",
    "\n",
    "        weighted_input = x*weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1)\n",
    "\n",
    "        def get_output_shape_for(self, input_shape):\n",
    "            return (input_shape[0], input_shape[-1])\n",
    "    '''\n",
    "    \n",
    "    # Hierarichal model\n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "    '''\n",
    "    l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "    l_dense = TimeDistributed(Dense(200))(l_lstm)\n",
    "    l_att = AttLayer()(l_dense)\n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "    '''\n",
    "    l_lstm = Bidirectional(LSTM(16))(embedded_sequences)\n",
    "    sentEncoder = Model(sentence_input, l_lstm)\n",
    "\n",
    "    #review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "    review_encoder = TimeDistributed(sentEncoder)(item_desc_input_layer)\n",
    "\n",
    "    l_lstm_sent = Bidirectional(LSTM(8))(review_encoder)\n",
    "\n",
    "    '''\n",
    "    l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "    l_dense_sent = TimeDistributed(Dense(200))(l_lstm_sent)\n",
    "    l_att_sent = AttLayer()(l_dense_sent)\n",
    "    '''\n",
    "    #return l_att_sent\n",
    "    return l_lstm_sent\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[MAX_SENTS, MAX_SENT_LENGTH], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    category_name = Input(shape=[1], name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_name = Embedding(MAX_TEXT, 50)(name)\n",
    "    #emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    #rnn layer\n",
    "    HATT_layer = get_hierarichal_representation(item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_name)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_category_name)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , HATT_layer\n",
    "        , rnn_layer2\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand_name\n",
    "                   , category_name, item_condition, num_vars], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()\n",
    "'''\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1467709 samples, validate on 14826 samples\n",
      "Epoch 1/5\n",
      " 720000/1467709 [=============>................] - ETA: 16:07 - loss: 0.0348 - mean_absolute_error: 0.1438 - rmsle_cust: 0.0165"
     ]
    }
   ],
   "source": [
    "#FITTING THE MODEL\n",
    "BATCH_SIZE = 20000\n",
    "epochs = 5\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, dvalid.target)\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "#Source: https://www.kaggle.com/marknagelberg/rmsle-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EVLUEATE THE MODEL ON DEV TEST: What is it doing?\n",
    "val_preds = model.predict(X_valid)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "#mean_absolute_error, mean_squared_log_error\n",
    "y_true = np.array(dvalid.price.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "print(\" RMSLE error on dev test: \"+str(v_rmsle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CREATE PREDICTIONS\n",
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = target_scaler.inverse_transform(preds)\n",
    "preds = np.exp(preds)-1\n",
    "\n",
    "submission = test[[\"test_id\"]]\n",
    "submission[\"price\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"./myNNsubmission.csv\", index=False)\n",
    "submission.price.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "\n",
    "https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
