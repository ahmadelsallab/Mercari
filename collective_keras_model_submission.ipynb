{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5666d5d1eade250bc4206790973f14f34f30aea"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a04fc38952a723488f8dcdd59b86575cf0fd6ad9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import math\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc6be66f620db46760641536ed3d24ee293a8b64"
   },
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da869174e159094ab5b5413103eed87dee72b27c"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "39507d6aa8f898d0d63829a1ed9d230cd246c901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded\n",
      "(1482535, 8)\n",
      "(693359, 7)\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "print(\"Loading data...\")\n",
    "import subprocess\n",
    "train = pd.read_table('../../dat/train.tsv')\n",
    "test = pd.read_table('../../dat/test.tsv')\n",
    "print('Data loaded')\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79b294194456deef54adfe104e2d29c5aae759eb"
   },
   "source": [
    "## Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "77c655d2f4bb7d568a541be5ac6be15b09022227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing values...\n",
      "(1482535, 8)\n",
      "(693359, 7)\n"
     ]
    }
   ],
   "source": [
    "#HANDLE MISSING VALUES\n",
    "print(\"Handling missing values...\")\n",
    "def handle_missing(dataset):\n",
    "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n",
    "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n",
    "    return (dataset)\n",
    "\n",
    "train = handle_missing(train)\n",
    "test = handle_missing(test)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "646b26406e3565b5ffefa197456db0be9dd525c9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ff709ed5a5ed3052a714d0182c3affec7693a02"
   },
   "source": [
    "## Handle categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cf3644210240e09bb0a01dbf0752bf12b312892",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PROCESS CATEGORICAL DATA\n",
    "print(\"Handling categorical variables...\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(np.hstack([train.category_name, test.category_name]))\n",
    "train.category_name = le.transform(train.category_name)\n",
    "test.category_name = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name, test.brand_name]))\n",
    "train.brand_name = le.transform(train.brand_name)\n",
    "test.brand_name = le.transform(test.brand_name)\n",
    "del le\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47ebd3b954a11dc6023403da5881a45933b1e06f"
   },
   "source": [
    "## Handle text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "dc0ea3f62298588a1b87781516eb652e1592a862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to seq process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fitting tokenizer...\n",
      "   Transforming text to seq...\n",
      "(1482535, 8)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8aee2ab1d6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seq_item_description\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#PROCESS TEXT: RAW\n",
    "print(\"Text to seq process...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "raw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n",
    "\n",
    "print(\"   Fitting tokenizer...\")\n",
    "tok_raw = Tokenizer()\n",
    "tok_raw.fit_on_texts(raw_text)\n",
    "print(\"   Transforming text to seq...\")\n",
    "\n",
    "\n",
    "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n",
    "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n",
    "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n",
    "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64a2eef1e9963070890ea78b85b3d1d80e34cb1f"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2793f78cc987c3028f06687d7ec81ed1df1752fb"
   },
   "source": [
    "### Embeddings size calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "274412036573ea189836cfde658f17608d13a0f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SEQUENCES VARIABLES ANALYSIS\n",
    "max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\n",
    "max_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n",
    "                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\n",
    "print(\"max name seq \"+str(max_name_seq))\n",
    "print(\"max item desc seq \"+str(max_seq_item_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3c19c9688a7d9485fb7f4c5db140ff4a5feb423",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EMBEDDINGS MAX VALUE\n",
    "#Base on the histograms, we select the next lengths\n",
    "MAX_NAME_SEQ = 10\n",
    "MAX_ITEM_DESC_SEQ = 75\n",
    "MAX_TEXT = np.max([np.max(train.seq_name.max())\n",
    "                   , np.max(test.seq_name.max())\n",
    "                  , np.max(train.seq_item_description.max())\n",
    "                  , np.max(test.seq_item_description.max())])+2\n",
    "MAX_CATEGORY = np.max([train.category_name.max(), test.category_name.max()])+1\n",
    "MAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\n",
    "MAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "950bb47a76ba584ec72eaaca444cbc99fd7a0b56"
   },
   "source": [
    "## Pre-trained embeddings (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1da038a73bda4dc153052e454c5ac3a94bf3f713"
   },
   "source": [
    "### Glove (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "069c9966c4b0f95ab281a92694117b63ebb68738"
   },
   "source": [
    "## Target formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b114f16679e4684022287ff00298ff13ee985d4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCALE target variable\n",
    "train[\"target\"] = np.log(train.price+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\n",
    "pd.DataFrame(train.target).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2bfb82f40b453df9df018742bad3779c1168d04d"
   },
   "source": [
    "## Train/Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05a244612b1fbef12b582ef6dfb0cb4ab150a7b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da41c792a1489bcca8524ff96d53142bcc850079",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KERAS DATA DEFINITION\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def get_keras_data(dataset):\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n",
    "        ,'brand_name': np.array(dataset.brand_name)\n",
    "        ,'category_name': np.array(dataset.category_name)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n",
    "    }\n",
    "    return X\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d876b0381e95ba8a85687d85153593a041e92ab"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd30313c3369dd9199ecf1310eaff2d549e554f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "\n",
    "def rmsle_cust(y_true, y_pred):\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = 0.1\n",
    "    \n",
    "    #Inputs\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\")\n",
    "    category_name = Input(shape=[1], name=\"category_name\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers\n",
    "    emb_name = Embedding(MAX_TEXT, 50)(name)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n",
    "    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n",
    "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n",
    "    \n",
    "    #rnn layer\n",
    "    rnn_layer1 = GRU(16) (emb_item_desc)\n",
    "    rnn_layer2 = GRU(8) (emb_name)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name)\n",
    "        , Flatten() (emb_category_name)\n",
    "        , Flatten() (emb_item_condition)\n",
    "        , rnn_layer1\n",
    "        , rnn_layer2\n",
    "        , num_vars\n",
    "    ])\n",
    "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n",
    "    \n",
    "    #output\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand_name\n",
    "                   , category_name, item_condition, num_vars], output)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()\n",
    "'''\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1cea5a48970d9576b927806c9cc7dfca9da1b7b"
   },
   "source": [
    "# Train/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17e0f5e011b972378bd591c02119992cbc29d86b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FITTING THE MODEL\n",
    "BATCH_SIZE = 20000\n",
    "epochs = 5\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, dvalid.target)\n",
    "          , verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8464e5ddf573a9a6d532d0ddca6d010173e5244"
   },
   "source": [
    "## KFold (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae2f914f60878b4399f51ae602e816d8ab76b17b"
   },
   "source": [
    "# Evaulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b8556c60a01d63202a454c28d13a91064306427",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n",
    "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n",
    "#Source: https://www.kaggle.com/marknagelberg/rmsle-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84bc765e7edc1492a30aec5a1b82f5f03c89f3d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EVLUEATE THE MODEL ON DEV TEST: What is it doing?\n",
    "val_preds = model.predict(X_valid)\n",
    "val_preds = target_scaler.inverse_transform(val_preds)\n",
    "val_preds = np.exp(val_preds)+1\n",
    "\n",
    "#mean_absolute_error, mean_squared_log_error\n",
    "y_true = np.array(dvalid.price.values)\n",
    "y_pred = val_preds[:,0]\n",
    "v_rmsle = rmsle(y_true, y_pred)\n",
    "print(\" RMSLE error on dev test: \"+str(v_rmsle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da360eda3bcac61037d84ebb16e6ddb4f90656f8"
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90607ea1d4d2cdaa2e2da01858682b22aa5c1bab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CREATE PREDICTIONS\n",
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = target_scaler.inverse_transform(preds)\n",
    "preds = np.exp(preds)-1\n",
    "\n",
    "submission = test[[\"test_id\"]]\n",
    "submission[\"price\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "927e49c08cc8ff266d3bbde8891ba307b5843554",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"./myNNsubmission.csv\", index=False)\n",
    "submission.price.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "babc413625dd92353df8f0488873965b4a2f7290"
   },
   "source": [
    "# Reference\n",
    "\n",
    "\n",
    "https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
