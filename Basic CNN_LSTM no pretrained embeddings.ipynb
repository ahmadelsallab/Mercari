{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.tests.test_cross_validation import test_train_test_split\n",
    "import pandas as pd\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 100\n",
    "\n",
    "# Convolution\n",
    "filter_length = 2\n",
    "nb_filter = 10\n",
    "pool_length = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 5\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "test__train_split_ratio = 0.1# 0.1 means 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(corpus, labels):\n",
    "\t# Randomize the dataSet\n",
    "\trandom_data = []\n",
    "\trandom_labels = []\n",
    "\t\n",
    "\t# Sample indices from 0..len(corpus)\n",
    "\tsize = len(corpus)\n",
    "\trand_indices = random.sample(range(size), size)\n",
    "\t\n",
    "\t\n",
    "\t# Insert in the final dataset N=self.datasetSize random tweets from the rawData\n",
    "\tfor index in rand_indices:\n",
    "\t\trandom_data.append(corpus[index])\n",
    "\t\trandom_labels.append(labels[index])\n",
    "\t\n",
    "\t# Calculate the test set size\n",
    "\ttest_set_size = int(test__train_split_ratio * size)\n",
    "\t\n",
    "\t# The trainSet starts from the begining until before the end by the test set size \n",
    "\ttrain_set = random_data[0 : size - test_set_size]\n",
    "\ttest_set  = random_data[len(train_set) : size]\n",
    "\ttrain_set_labels = random_labels[0 : size - test_set_size]\n",
    "\ttest_set_labels  = random_labels[len(train_set) : size]\t\n",
    "\treturn train_set, train_set_labels, test_set, test_set_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_to_indices(text):\n",
    "\t#words_map = build_words_map(text)\n",
    "\t\t\n",
    "\t#text_to_indices(text, words_map)\n",
    "\n",
    "\t# The vocabulary map\n",
    "\twords_map = {}\n",
    "\n",
    "\t# Index of words\n",
    "\tindex = 0\n",
    "\t\n",
    "\t# Initialize the output list\n",
    "\ttext_indices = []\n",
    "\tmaxlen = 0\n",
    "\t# Loop line by line\n",
    "\t\n",
    "\tfor line in text:\n",
    "\n",
    "\t\t# Split into words\t\n",
    "\t\ttry:\t\t\n",
    "\t\t\tline = str(line)\n",
    "\t\t\tline_words = line.split()\n",
    "\t\texcept:\n",
    "\t\t\tprint(str(line))\n",
    "\t\n",
    "\t\tif len(line_words) > maxlen:\n",
    "\t\t\tmaxlen = len(line_words) \n",
    "\t\t# Initialize the line_indices\n",
    "\t\tline_indices = []\n",
    "\t\t# Loop word by word\n",
    "\t\tfor word in line_words:\n",
    "\t\t\t# Store the word once in the wordMap\n",
    "\t\t\tif not word in words_map:\n",
    "\t\t\t\twords_map[word] = index\n",
    "\t\t\t\t# Increment the index for the next word\n",
    "\t\t\t\tindex += 1\n",
    "\n",
    "\t\t\t# Add the index to the line_indices\n",
    "\t\t\tline_indices.append(words_map[word])\n",
    "\n",
    "\t\t# Add the line_indices to the output list\n",
    "\t\ttext_indices.append(line_indices)\n",
    "\n",
    "\n",
    "\treturn text_indices, len(words_map), maxlen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "\t# Load training data frame\n",
    "\ttrain_data_path = '../../dat/train.tsv'\n",
    "\ttrain_df = pd.read_table(train_data_path)\n",
    "\tprint('Training DataFrame loaded')\n",
    "\t\n",
    "\ttext = train_df['item_description']\n",
    "\ttext_indices, voc_size, maxlen = corpus_to_indices(text)\n",
    "\t\n",
    "\t# Load labels\n",
    "\tlabels = np.array(train_df['price'])\n",
    "\t\n",
    "\tX_train, y_train, X_test, y_test = split_train_test(text_indices, labels) \t\n",
    "\treturn X_train, y_train, X_test, y_test, voc_size, maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "X_train, y_train, X_test, y_test, max_features, maxlen = load_data()\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "y_train = np.reshape(y_train, [len(y_train), 1])\n",
    "y_test = np.reshape(y_test, [len(y_test), 1])\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "model.add(LSTM(lstm_output_size))\n",
    "\n",
    "model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "\n",
    "import keras.optimizers\n",
    "opt = keras.optimizers.adam(0.01)\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=opt)\n",
    "              #metrics=['accuracy'])\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "   \n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,callbacks=callbacks_list,verbose=0,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "'''\n",
    "model.save('atb_model', overwrite=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
